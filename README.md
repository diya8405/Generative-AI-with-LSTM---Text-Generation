Shakespeare Text Generation with LSTM
A deep learning project that generates Shakespeare-style text using LSTM (Long Short-Term Memory) neural networks. The model is trained on Shakespeare's complete works and can generate coherent text sequences in his distinctive style.

ğŸ“‹ Table of Contents

Overview
Features
Dataset
Model Architecture
Requirements
Installation
Usage
Results
Model Performance
Contributing
License

ğŸ­ Overview
This project implements a character-level text generation model using LSTM networks to create Shakespeare-style text. The model learns patterns from Shakespeare's complete works and generates new text sequences that mimic his writing style.
âœ¨ Features

Text Preprocessing: Cleans and tokenizes Shakespeare's text
LSTM-based Architecture: Uses stacked LSTM layers for sequence learning
Word-level Generation: Generates text word-by-word
Pre-trained Model: Includes saved model weights
Customizable Generation: Generate text of any desired length

ğŸ“š Dataset
The model is trained on Shakespeare's complete works from MIT OpenCourseWare:
Dataset URL: https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt
Dataset Statistics:

Total tokens: ~898,199 words
Unique words: ~27,956
Training sequences: ~199,951

ğŸ—ï¸ Model Architecture
Model: "sequential"
_________________________________________________________________
Layer (type)                Output Shape              Param #   
=================================================================
embedding (Embedding)       (None, 50, 50)            650,450    
lstm (LSTM)                 (None, 50, 100)           60,400     
lstm_1 (LSTM)               (None, 100)               80,400     
dense (Dense)               (None, 100)               10,100     
dense_1 (Dense)             (None, 13009)             1,313,909   
=================================================================
Total params: 2,115,259 (8.07 MB)
Trainable params: 2,115,259 (8.07 MB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
Key Components:

Embedding Layer: 50-dimensional word embeddings
LSTM Layers: Two stacked LSTM layers (100 units each)
Dense Layers: Fully connected layers for prediction
Output: Softmax activation over vocabulary

ğŸ”§ Requirements
tensorflow>=2.15.0
keras>=2.15.0
numpy
matplotlib
requests
ğŸ“¥ Installation
1. Clone the Repository
bashgit clone https://github.com/diya8405/Generative-AI-with-LSTM---Text-Generation.git
cd shakespeare-text-generation
2. Create Virtual Environment (Optional but Recommended)
bash# Using venv
python -m venv venv

# Activate on Windows
venv\Scripts\activate

# Activate on macOS/Linux
source venv/bin/activate
3. Install Dependencies
bashpip install -r requirements.txt
```

**requirements.txt**:
```
tensorflow==2.15.0
keras==2.15.0
numpy
matplotlib
requests
protobuf==3.20.3
ğŸš€ Usage
Training the Model
python# Run the complete training pipeline
python train.py

# Or use the Jupyter notebook
jupyter notebook generative-ai-with-lstm-text-generation.ipynb
Generating Text
pythonfrom tensorflow.keras.models import load_model
import pickle

# Load the trained model
model = load_model('shakespeare_bot.h5')

# Load tokenizer
with open('tokenizer.pkl', 'rb') as f:
    tokenizer = pickle.load(f)

# Generate text
seed_text = "to be or not to be"
generated_text = generate_text_seq(model, tokenizer, 50, seed_text, 100)
print(generated_text)
Quick Start with Pre-trained Model
python# Load and use the pre-trained model
from model_utils import load_shakespeare_model, generate_shakespeare_text

model, tokenizer = load_shakespeare_model()
text = generate_shakespeare_text(model, tokenizer, "fair maiden", num_words=50)
print(text)
```

## ğŸ“Š Results

### Training Configuration
- **Sequence Length**: 50 words
- **Batch Size**: 256
- **Epochs**: 5 (recommended: 250 for better accuracy)
- **Optimizer**: Adam
- **Loss Function**: Categorical Crossentropy

### Sample Generated Text

**Input Seed**: 
```
"home of love if i have ranged like him that travels i return again"
```

**Generated Output** (with 5 epochs):
```
"not a man and the world and the world and the world..."
```

*Note: With more training epochs (100-250), the model generates more coherent and Shakespeare-like text with ~40% accuracy.*

## ğŸ“ˆ Model Performance

### Training Metrics

- **Initial Loss**: ~9.0
- **Final Loss**: ~6.5 (after 5 epochs)
- **Initial Accuracy**: ~5%
- **Final Accuracy**: ~10% (after 5 epochs)

### Performance Notes

- âš ï¸ **5 epochs**: Basic patterns, repetitive output
- âœ… **100 epochs**: ~40% accuracy, better coherence
- ğŸ¯ **250 epochs**: Recommended for production-quality results

## ğŸ› ï¸ Project Structure
```
shakespeare-text-generation/
â”‚
â”œâ”€â”€ generative-ai-with-lstm-text-generation.ipynb  # Main notebook
â”œâ”€â”€ train.py                                        # Training script
â”œâ”€â”€ model_utils.py                                  # Utility functions
â”œâ”€â”€ shakespeare_bot.h5                              # Trained model
â”œâ”€â”€ tokenizer.pkl                                   # Saved tokenizer
â”œâ”€â”€ requirements.txt                                # Dependencies
â”œâ”€â”€ README.md                                       # This file
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ shakespeare.txt                             # Dataset (auto-downloaded)
â”‚
â””â”€â”€ models/
    â””â”€â”€ checkpoints/                                # Model checkpoints
ğŸ”„ Git Commands Reference
Initial Setup
bash# Initialize repository
git init
git add .
git commit -m "Initial commit: Shakespeare LSTM text generator"
git branch -M main
git remote add origin https://github.com/diya8405/Generative-AI-with-LSTM---Text-Generation.git
git push -u origin main
Regular Updates
bash# Check status
git status

# Add changes
git add .

# Commit changes
git commit -m "Your descriptive message"

# Push to remote
git push origin main

# Pull latest changes
git pull origin main
Working with Branches
bash# Create new branch
git checkout -b feature/new-feature

# Switch branches
git checkout main

# Merge branch
git merge feature/new-feature

# Delete branch
git branch -d feature/new-feature
ğŸ¤ Contributing
Contributions are welcome! Please follow these steps:

Fork the Repository

bash# Click 'Fork' on GitHub, then:
git clone https://github.com/diya8405/Generative-AI-with-LSTM---Text-Generation.git

Create a Feature Branch

bashgit checkout -b feature/AmazingFeature

Commit Changes

bashgit commit -m "Add: Amazing new feature"

Push to Branch

bashgit push origin feature/AmazingFeature

Open a Pull Request

ğŸ“ To-Do

 Implement character-level generation
 Add temperature parameter for creativity control
 Create web interface for text generation
 Add beam search decoding
 Implement attention mechanism
 Support for multiple authors/styles

âš ï¸ Known Issues

Model requires significant training time (5 epochs â‰ˆ 15 minutes on GPU)
Low-epoch models produce repetitive output
Large model size (8.07 MB)

ğŸ“„ License
This project is licensed under the MIT License - see the LICENSE file for details.
ğŸ™ Acknowledgments

Shakespeare's texts provided by MIT OpenCourseWare
Built with TensorFlow and Keras
Inspired by Andrej Karpathy's char-rnn

ğŸ“§ Contact
Diya Kansagara - diyakansagara25@gmail.com
Project Link: https://github.com/diya8405/Generative-AI-with-LSTM---Text-Generation

Made with â¤ï¸ and Shakespeare's wisdom
